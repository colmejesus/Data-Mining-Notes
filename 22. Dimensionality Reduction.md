## It is a data mining technique which involves reducing the number of inputs variables or features or representing the data in an aggregated/summarized format while preserving as much relevant information as possible, not hampering the data quality. 

## This is done to improve the computation required to process the data set, remove noise from the dataset or to reduce redundant (unimportant) features from the dataset, which also leads to computational efficiency, as the result after the dimensionality reduction is almost the same but the computation time to reach that result is decreased, hence increasing efficiency. 

## When the size of the data set is smaller, it is much more efficient to apply algorithms that are computationally costly without compromising on the quality of the data. 

## There are three common Data Reduction Techniques:

#### 1. Dimensionality Reduction: It is a technique that specifically deals with reducing the number of input variables or features from a dataset, common methods include PCA, autoencoders etc. 

#### 2. Data Cube Aggregation: Data cube aggregation is a process of summarizing and consolidating large amounts of multidimensional data into a more compact form. It involves organizing data into a data cube, which is a multidimensional representation of the data, where each cell in the cube represents a particular combination of the dimensions. Aggregation then involves computing summary statistics, such as averages or sums, over subsets of the data cube based on the values of one or more of its dimensions. The result is a condensed representation of the data that can be queried and analyzed more efficiently. Data cube aggregation is commonly used in data warehousing and OLAP (Online Analytical Processing) applications.

#### Picture showing aggregation being done on a data cube that comprising sales figures, per year: 
![](https://static.javatpoint.com/tutorial/data-mining/images/data-reduction-in-data-mining3.png)


#### 3. Data Compression: Data compression is a technique used in dimensionality reduction that converts the structure of features so that they consume less space in the memory. It helps in building a compact representation of information by removing redundancy and representing data in binary form. There are two kinds of compression:
- Lossless Compression: In this kind of compression, the compressed data can be reverted to its original form using various algorithms.
- Lossy Compression: In this kind of compression, the compressed data cannot be reverted to it's original form. 

## Three of the most common methods of Dimensionality reduction are:

#### 1. Wavelet Transform: It is a technique that breaks down a dataset into small pieces, called wavelets. Each wavelet captures a different feature of the original data point at different scale. By analyzing different wavelets, we can get an idea about the most important information. This information can be used for dimensionality reduction by removing the less important features and only keeping the features that are most relevant to us, which can help us understand the data in a more efficient way. 

#### 2. Principal Component Analysis: Principal component analysis (PCA) is a technique that transforms high-dimensions data, i.e. data with a lot of features, into lower-dimensions while retaining as much information as possible. PCA helps in identifying key attributes of a dataset, after identifying the key attributes we can remove the redundant ones allowing us to represent the dataset using far fewer attributes without compromising with the quality of the data. 

#### Scatter plot of a Dataset With 3 Dimension (3 features): 

![Original_dataset_with_3_features](https://miro.medium.com/max/640/1*QinDfRawRskupf4mU5bYSA.webp)

#### Scatter plot after PCA, dataset reduced from 3-dimensions (3 features) to 2 dimensions:

![scatterplot_3d to 2d](https://miro.medium.com/max/4800/1*LKTwaVmP4Dqxb-N3iD3CHw.webp)


#### Here's a great article explaining PCA in detail: [Principal Component Analysis (PCA) Explained Visually with Zero Math](https://towardsdatascience.com/principal-component-analysis-pca-explained-visually-with-zero-math-1cbf392b9e7d)


#### 3. Attribute Subset Selection: In machine learning and data mining, attribute subset selection is a process of selecting a subset of features from a larger set of features in a dataset that are relevant to a particular task or problem. The features, also known as attributes or variables, are the characteristics or properties of the objects or samples in the dataset. Examples of features could be the height, weight, age, gender, income, education, etc. of a group of people or the pixel intensity, color, texture, shape, etc. of an image.

####  The main goal of attribute subset selection is to identify the most informative and discriminative features that are necessary and sufficient to represent the underlying patterns or relationships in the data, while minimizing the redundancy and noise. By reducing the dimensionality of the data, attribute subset selection can help to simplify the analysis, improve the interpretability and visualization of the results, and enhance the efficiency and accuracy of machine learning algorithms. There are different methods and algorithms to perform attribute subset selection, including filter methods, wrapper methods, and embedded methods. For example, filter methods are based on statistical or information-theoretic measures to evaluate the relevance and importance of each feature independently of the learning algorithm. Examples of filter methods include chi-squared test, mutual information, correlation coefficient, information gain, and variance threshold.

