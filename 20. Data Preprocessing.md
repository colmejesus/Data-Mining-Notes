## Data preprocessing refers to the techniques used to transform raw data into clean and usable format for analysis, modeling, and deployment. It is a crucial step in data science workflow and involves a number of tasks such as:

1. Data Cleaning: Involves identifying missing values, outliers, inconsistencies, redundancies in a data set. 
2. Data Normalization: Process of transforming variables to a standard scale to handle differences in units and measurements.
3. Data Integration: Process that involves combining data from various sources, into a single unified dataset. 
4. Data Transformation: Process of converting data into a different format or structure that is more suitable for analysis or modeling. 
5. Data Reduction (Dimensionality Reduction): Process of reducing the amount of data by aggregating or summarizing it.

## Quality of Data:

Quality of data refers to the level of accuracy, consistency, completeness, believability, Interpretability of the data used for analysis and modeling. High-quality data is essential for accurate data modeling and analysis. 

Reasons for inaccurate data:

1.  Human Error: One of the main reasons for inaccurate, incomplete, and inconsistent data is human error. This can occur during data entry, data collection, or data recording processes.
    
2.  Technical Issues: Technical issues such as software bugs, hardware failures, or network disruptions can cause data to be incorrect, incomplete, or inconsistent.
    
3.  Inadequate Data Collection: Poorly designed data collection methods or insufficient resources can result in incomplete or inconsistent data.
    
4.  Inaccurate Sensors or Instruments: Inaccurate sensors or instruments can result in incorrect data. For example, if a measuring device is not calibrated correctly, the data it collects will be inaccurate.
    
5.  Data Integration: Integrating data from multiple sources can result in inconsistencies or missing values if the data sources are not compatible or if the data is not properly mapped.
    
6.  Data Corruption: Data can become corrupt due to software or hardware issues, power outages, or other unexpected events.
    
7.  Data Privacy and Security: Data privacy and security concerns can result in data being removed or altered, which can impact the accuracy and completeness of the data.

## What are believability and interpretability in data?

Believability refers to how much the data that is being used to model or for analysis can be trusted? If the data comes from an unreliable source, it can lead to misleading insights and inaccurate predictions, especially n sensitive areas such as finance, healthcare or national security as data mining algorithms rely on accurate data to make accurate predictions and to uncover patterns and relationship between data and if the data is inaccurate and untruthful the results of the mining process will be unreliable as well. 

Interpretability: It refers to the ease with which the data or the result of a data analysis can be understood and acted upon, in other words, it is the measure for how clear and understandable the results are. The results of a data mining algorithm must be presented in a way that is easily understood by the stakeholders to make informed decisions based on those results, which can impact the value of data mining efforts. 

## Data Cleaning:

'Dirty' i.e. inaccurate, inconsistent, missing or redundant data can hamper the data mining process, making the result of a mining algorithm unreliable and inefficient. 

Some of the important methods of data cleaning when it comes to handling missing values in a data set involves:
 1. Tuple Ignoring: Pretty self-explanatory, involves ignoring/removing all the tuples where most of the attributes are missing. It is not a very good method to follow when the percentage of missing values is too high. 
 2. Data imputation using mean or median values: This method demands the user to fill the missing tuple with the measure of central tendency of the remaining available values, i.e. mean or median. Mean can be used for normally distributed data whereas if the data is skewed, it is a good practice to use median. 
 3. Manual filling: Involves manually filling each missing tuple, it is very time-consuming for large datasets
 4. Using Global constant: Filling the missing tuples with keyword like 'Unknown' keyword or symbols like 'âˆž'