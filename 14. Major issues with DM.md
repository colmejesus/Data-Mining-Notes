## User Interaction: 

## User plays an important role in data mining process, it is important to make the data mining tools to be as user-friendly as possible, to the extent that non-technical users that handle the business side of an organization can easily interact with data sets and use them to mine useful information. 

## Some of the interesting areas of research in this field include, how users interact with a data mining system, how to incorporate a user's background knowledge in mining to help them use the tools more efficiently and how easily let users visualize and comprehend data mining results. 

## To make the user interaction efficient we need to build flexible interfaces and exploratory mining environment, this will help user interact effortlessly with the mining system hence leading users to make faster, more accurate business decisions.

## Interactive mining should allow users to dynamically change the focus of a search and to refine mining queries/requests based on the output they get back from their initial queries and also to 'drill, dice and pivot' through the data and knowledge space interactively and efficiently, here's what those three terms mean in context of data mining:

### 1. Drilling down: The process of focusing on  exploring deeper into the details of the data by focusing on specific aspects or attributes.
### 2. Dicing: The process of dividing the data set into smaller parts based on the values of specific attributes of the data, i.e. Extracting information about the students who scored above 80% in an exam. 
### 3. Pivoting: It's the process of rearranging data in a way that allows you to see things from a different perspective. This can be useful for summarizing and aggregating the data in a way that is easier to analyze and understand.

### Let's understand these concepts using examples:

#### Imagine you have a data set that contains information about sales at a retail store. The data includes information about the products sold, the date of the sale, the department the product was in, and the customer who made the purchase.

#### Drilling down:

#### Suppose you're interested in understanding sales of a specific product, say, a particular brand of shoes. To drill down, you would focus your analysis on just the rows of the data set that relate to that brand of shoes. You might look at sales by month or by department to get a better understanding of how well the product is selling.

#### Dicing:

#### Now, suppose you're interested in understanding sales by department. To dice the data, you would separate the data set into smaller sets based on the department the product was in. For example, you might create a separate data set for shoes, another for clothing, and another for electronics. You can then analyze each data set separately to see how sales vary across departments.

#### Pivoting:

#### Imagine you have a table of data showing the total sales for each department in the store over a period of time, with the departments as rows and the dates as columns. To pivot the data, you would rearrange it so that the dates are the rows and the departments are the columns. This would give you a view of sales over time for each department, making it easier to compare sales trends across departments and see how they vary over time.

##### For example, if the original table looked like this:

![[pivoting example.png]]

##### After pivoting, the table would look like this:

![[after_pivoting_example.png]]

#### This view of the data makes it easier to see how sales for each department have changed over time, and to compare the trends for different departments.

## An easy-to-use data mining interface can incorporate the user's background knowledge of a domain more efficiently by providing them with a visual and intuitive interface that allows the user to interact with the data in a way that leverages their domain knowledge. Some key ways this can be achieved is:

### 1. Predefined filters and queries: The user interface could provide pre-defined queries specific to the user's domain, allowing them to quickly and easily explore data in a way that is relevant to their area of expertise.

### Visualization tools: The interface could provide visualization tools that are designed to be intuitive and easy to use, allowing the user to quickly and easily see patterns and relationships in the data that is relevant to their area of expertise. 

### 3. Customizable Dashboards: The interface could allow users to create customizable dashboards that display data in a way that is relevant to their area of expertise, making it easier for them to quickly see the information they need, and it lets them easily group and communicate that information to others that makes it easier for other people to understand. Here's how a data dashboard looks like:

![[analytics-home-dashbboard-new.png]]

## Ad-hoc data mining and mining query languages:

### Ad-hoc is Latin for 'for this purpose'. You might cal it 'on the fly', or a 'just so' query. It's the kind of SQL query you loosely type out when you need it. For e.g.:

```sql
var newSqlQuery = "SELECT * FROM table WHERE id = " + myId;
```

### Which can be an entirely different query, every time you execute it, based on the value of `myID`. 

### SQL has played an important role in data mining because it lets users execute these type of ad-hoc queries, making the searching process more flexible. Similarly, high-level data mining query languages or other high-level flexible user-interfaces will give users the freedom to define ad-hoc data mining tasks without having a deep understanding of the underlying technical details of the data mining process. 

### For example, a high-level command can let users specify a complex data mining task using natural language like-commands rather than having to write complex code. This makes the data mining process more accessible for users who may not have a strong technical background by letting them specify their queries on the data sets and the domain knowledge they want to incorporate using simple natural language commands like mentioned above and letting them set conditions and constraints i.e. the parameters of their search in their queries much more easily, making the whole process easier in general. 

## Efficiency and Scalability:

### Effective and scalable data mining algorithms are always considered while talking about processing very large sets of data, as data sets continue to grow bigger and bigger, these two factors are becoming much more critical to having a faster, more reliable data mining system. 

### Data mining algorithms must be efficient and scalable in order to effectively extract information from huge amounts of data, that could be structured or unstructured and can be distributed across many data repositories or could be a dynamic stream of data i.e. continuous flow of data that changes over time, this data may come from sources like sensors, social media, financial transactions etc. 

### In other words, the running time of data mining algorithms must be short, predictable and should be able to process data streams in real-time. 

### The huge size of data sets, the wide distribution of data across many data repositories and the computational complexity of some data mining methods are the factors that motivate, parallel (type of algo that uses mulitple CPU cores to parallelly process data by dividing it into smaller chunks, hence taking advantage of full computing power of a system) and distributive, data-intensive mining algorithms.  

### The parallel processing algorithm, like stated above, uses mulitple CPU cores to parallelly process data by dividing it into smaller chunks and each process may interact with each other, the patterns found in each process are eventually merged. 

### Cloud and cluster computing, both use computers in a distributed way i.e. many computers used parallelly across a network, to tackle large-scale computational tasks, are also topics of active research in data mining. 

### Incremental data mining: is a process that lets a user update a data mining model, they created,  i.e. this could be a model that predicts marks based on the number of hours studied and previous test scores, by students, with new data, without having to re-train the entire data set with additional new data added to it. This saves a lot of compute power, especially if the data set is very big in size.

## Diversity in Database type:

### Wide variety of database types bring about challenges to the data mining process.

### It is difficult to handle complex types of data such as audio, images, videos etc. As nowadays there is are many diverse sets of application such as social media, video streaming platforms that generate wide spectrum of data mentioned above, also the data from Data warehouses, dynamic data streams, it is much more complicated to process and make sense of data with the advent of these technologies.

### The discovery of knowledge from different sources of data such as relational databases (structured data), data warehouses (unstructured data and structured data) having diverse data semantics (different data points that have different meaning) poses a great challenge to data mining. 

## Mining these diverse data sets combined may disclose many more patterns and hidden information when compared to data from small isolated data repositories. 

### Data mining and Society:

### With the advent of so many tools that lets us collect and analyze data, the improper disclosure or use of data may cause potential violation of an individual privacy. Data protection rights are areas of concern associated with data mining right now.

### Although, data mining is also helping the society with scientific discovery, business management, economy recovery and security protection (real-time discovery of intruders and cyberattacks using outlier detection and other mining techniques) However data mining if carried out in an irresponsible manner poses a risk of disclosing an individual's personal information and studies on privacy preserving data publishing and data mining are on going to mitigate privacy risks, the philosophy is to preserve people's privacy while performing data successful data mining.

